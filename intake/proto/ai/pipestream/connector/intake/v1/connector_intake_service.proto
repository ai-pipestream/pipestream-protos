syntax = "proto3";

package ai.pipestream.connector.intake.v1;

import "ai/pipestream/connector/v1/connector_types.proto";
import "ai/pipestream/data/v1/pipeline_core_types.proto";
import "google/protobuf/struct.proto";
import "google/protobuf/timestamp.proto";

option java_multiple_files = true;
option java_package = "ai.pipestream.connector.intake.v1";

// ============================================
// CONNECTOR-INTAKE-SERVICE
// Handles authentication, account lookup, metadata enrichment
// Agnostic to connector type (filesystem, database, API, etc.)
// ============================================

// High-throughput document ingestion service for all connector types.
//
// This service provides the main entry point for document ingestion into the AI Pipestream platform.
// It handles authentication, account verification, metadata enrichment, and routing to the repository layer.
// Supports both structured PipeDoc uploads and raw blob uploads with automatic wrapping.
service ConnectorIntakeService {
  // 1. The "Power User" path: Client sends a full PipeDoc
  // Best for internal services, advanced connectors, or "Box.com" style integrations
  // where the client already knows the structure.
  rpc UploadPipeDoc(UploadPipeDocRequest) returns (UploadPipeDocResponse);

  // 2. The "Simple" path: Client sends raw bytes (Blob) + minimal metadata
  // Best for "I just have a file" scenarios. Intake constructs the PipeDoc wrapper.
  rpc UploadBlob(UploadBlobRequest) returns (UploadBlobResponse);

  // Register a new crawl session (Async/Background only)
  rpc StartCrawlSession(StartCrawlSessionRequest) returns (StartCrawlSessionResponse);

  // End a crawl session (for cleanup, orphan detection)
  rpc EndCrawlSession(EndCrawlSessionRequest) returns (EndCrawlSessionResponse);

  // Heartbeat to keep session alive
  rpc Heartbeat(HeartbeatRequest) returns (HeartbeatResponse);
}

// ============================================
// DATASOURCE ADMIN SERVICE
// Manages DataSources (account + connector type bindings)
// ============================================

// Administrative operations for datasource lifecycle management.
//
// A DataSource represents an account's binding to a connector type with its own API key.
// Connector types are pre-seeded templates (e.g., "s3", "file-crawler").
// DataSource = Account + Connector Type + API Key + Configuration.
service DataSourceAdminService {
  // Create a new datasource for an account
  rpc CreateDataSource(CreateDataSourceRequest) returns (CreateDataSourceResponse);

  // Update datasource configuration
  rpc UpdateDataSource(UpdateDataSourceRequest) returns (UpdateDataSourceResponse);

  // Get datasource details
  rpc GetDataSource(GetDataSourceRequest) returns (GetDataSourceResponse);

  // Validate API key for a datasource
  rpc ValidateApiKey(ValidateApiKeyRequest) returns (ValidateApiKeyResponse);

  // List all datasources for an account
  rpc ListDataSources(ListDataSourcesRequest) returns (ListDataSourcesResponse);

  // Enable/disable a datasource
  rpc SetDataSourceStatus(SetDataSourceStatusRequest) returns (SetDataSourceStatusResponse);

  // Delete a datasource (soft delete)
  rpc DeleteDataSource(DeleteDataSourceRequest) returns (DeleteDataSourceResponse);

  // Generate new API key for a datasource
  rpc RotateApiKey(RotateApiKeyRequest) returns (RotateApiKeyResponse);

  // Get crawl history for a datasource
  rpc GetCrawlHistory(GetCrawlHistoryRequest) returns (GetCrawlHistoryResponse);

  // List available connector types (pre-seeded templates)
  rpc ListConnectorTypes(ListConnectorTypesRequest) returns (ListConnectorTypesResponse);

  // Get connector type details
  rpc GetConnectorType(GetConnectorTypeRequest) returns (GetConnectorTypeResponse);
}

// ============================================
// UPLOAD MESSAGES
// ============================================

// Request to upload a fully-formed PipeDoc.
//
// Power user path for internal services and advanced connectors.
message UploadPipeDocRequest {
  // DataSource identifier (deterministic: hash of account_id + connector_id).
  string datasource_id = 1;
  // API key or JWT token for authentication.
  string api_key = 2;
  // Optional session ID for grouping related uploads.
  string session_id = 3;
  // The full document structure with all metadata and content.
  ai.pipestream.data.v1.PipeDoc pipe_doc = 4;
  // Source document identifier from the origin system.
  // Used to generate deterministic doc_id: hash(datasource_id + source_doc_id)
  string source_doc_id = 5;
}

// Request to upload raw file content with minimal metadata.
//
// Simple path for basic file uploads where the service constructs the PipeDoc wrapper.
message UploadBlobRequest {
  // DataSource identifier (deterministic: hash of account_id + connector_id).
  string datasource_id = 1;
  // API key for authentication.
  string api_key = 2;
  // Optional session ID for grouping related uploads.
  string session_id = 3;
  // Original filename.
  string filename = 4;
  // MIME type of the content.
  string mime_type = 5;
  // Virtual path in the datasource's namespace (e.g., "/folder/file.txt").
  string path = 6;
  // Additional metadata key-value pairs.
  map<string, string> metadata = 7;
  // Raw file content (up to 2GB).
  bytes content = 8;
  // Source document identifier from the origin system.
  // Used to generate deterministic doc_id: hash(datasource_id + source_doc_id)
  // If not provided, path or filename will be used.
  string source_doc_id = 9;
}

// Response from document upload operations.
message UploadPipeDocResponse {
  // Whether the upload succeeded.
  bool success = 1;
  // Document ID assigned by the repository service.
  string doc_id = 2;
  // Status or error message.
  string message = 3;
}

message UploadBlobResponse {
  // Whether the upload succeeded.
  bool success = 1;
  // Document ID assigned by the repository service.
  string doc_id = 2;
  // Status or error message.
  string message = 3;
}

// ============================================
// DATASOURCE CONFIG
// Simplified config returned after validation
// ============================================

// DataSource configuration returned after authentication.
// This is a lightweight view for runtime use (no secrets, no full Drive details).
message DataSourceConfig {
  // Account owning this datasource.
  string account_id = 1;
  // DataSource identifier (deterministic).
  string datasource_id = 2;
  // Connector type (e.g., "s3", "file-crawler").
  string connector_id = 3;
  // Drive name for storage (resolve via FilesystemService for S3 details).
  string drive_name = 4;
  // Maximum file size allowed in bytes.
  int64 max_file_size = 5;
  // Rate limit for uploads per minute.
  int64 rate_limit_per_minute = 6;
  // Metadata applied to all documents from this datasource.
  map<string, string> metadata = 7;
  // Tier 1: Global configuration (merged from Connector defaults + DataSource overrides).
  ConnectorGlobalConfig global_config = 8;

  // Global configuration for connector/datasource (Tier 1).
  //
  // Contains strongly typed configuration fields (persistence, retention, encryption, hydration)
  // and custom connector-specific configuration (JSON Schema-validated).
  message ConnectorGlobalConfig {
    // Strongly typed configuration fields (NOT JSON Schema).
    PersistenceConfig persistence_config = 1;
    RetentionConfig retention_config = 2;
    EncryptionConfig encryption_config = 3;
    // Hydration/dehydration policies - uses shared type from core types.
    ai.pipestream.data.v1.HydrationConfig hydration_config = 4;

    // Note: StorageConfig is NOT here - storage details are managed by repository-service
    // via the Drive entity (FilesystemService). DataSource in datasource-admin only has
    // drive_name (string reference). Drive contains: bucket_name, region, credentials_ref,
    // kms_encryption, s3_credentials, bucket_config, etc. (all managed by repository-service).

    // Custom connector-specific config (ONLY field validated against JSON Schema).
    // Examples: Wikipedia connector might have "parse_images", "include_history_metadata", "extract_urls"
    // This is the only JSON Schema-validated configuration.
    google.protobuf.Struct custom_config = 5;
  }

  // Configuration for document persistence decisions.
  message PersistenceConfig {
    // For HTTP uploads: always persisted (streaming requirement).
    // For gRPC PipeDoc uploads: configurable.
    // Default: true (safe default - always persist unless explicitly disabled).
    bool persist_pipedoc = 1;

    // Conditional persistence rules.
    int32 max_inline_size_bytes = 2; // Max size before forcing persistence (e.g., 1MB).

    // Future: Document type-based persistence rules.
    // map<string, bool> persist_by_document_type = 3;
  }

  // Configuration for document retention policies.
  message RetentionConfig {
    // Retention policy settings (strongly typed).
    RetentionPolicy policy = 1;
    int64 retention_days = 2; // -1 = indefinite, 0 = no retention, >0 = days.

    enum RetentionPolicy {
      RETENTION_POLICY_UNSPECIFIED = 0;
      RETENTION_POLICY_INDEFINITE = 1; // Keep forever.
      RETENTION_POLICY_TIME_BASED = 2; // Delete after retention_days.
      RETENTION_POLICY_VERSION_BASED = 3; // Keep N versions.
      RETENTION_POLICY_NO_RETENTION = 4; // Ephemeral (no archival).
    }

    // Version-based retention (if policy is VERSION_BASED).
    int32 max_versions = 3;
  }

  // Configuration for document encryption settings.
  message EncryptionConfig {
    // Encryption policy settings (strongly typed).
    EncryptionType encryption_type = 1;

    // Server-side encryption with KMS (SSE-KMS).
    // KMS key identifier - references key stored in KMS (or dev equivalent like localstack).
    // The actual key is stored/managed by KMS, not in this config.
    string kms_key_id = 2;

    // Client-provided encryption key (for SSE-C: Server-Side Encryption with Customer-Provided Keys).
    // Client provides the key, AWS/S3 uses it for encryption.
    // We store the key reference/identifier in KMS (or dev equivalent), not the key itself.
    ClientProvidedKeyConfig client_provided_key = 3;

    enum EncryptionType {
      ENCRYPTION_TYPE_UNSPECIFIED = 0;
      ENCRYPTION_TYPE_NONE = 1; // No encryption.
      ENCRYPTION_TYPE_SERVER_SIDE = 2; // Server-side encryption (SSE-S3).
      ENCRYPTION_TYPE_KMS = 3; // KMS-managed keys (SSE-KMS).
      ENCRYPTION_TYPE_CLIENT_PROVIDED = 4; // SSE-C: Client-provided keys (AWS uses for encryption).
    }

    message ClientProvidedKeyConfig {
      // Key identifier/reference stored in KMS (or dev equivalent like localstack).
      // The actual encryption key is provided by the client and managed by the storage provider.
      // We store only the key reference/identifier here.
      string key_reference_id = 1; // Reference to key stored in KMS/local KMS.

      // Algorithm used (e.g., "AES256").
      string algorithm = 2;
    }
  }
}

// ============================================
// CRAWL SESSION MANAGEMENT
// ============================================

// Request to start a new crawl session.
message StartCrawlSessionRequest {
  // DataSource identifier.
  string datasource_id = 1;
  // API key for authentication.
  string api_key = 2;
  // Client-provided crawl identifier.
  string crawl_id = 3;
  // Crawl metadata and parameters.
  CrawlMetadata metadata = 4;
  // Whether to track all documents in this crawl for orphan detection.
  bool track_documents = 5;
  // Whether to delete documents not seen in this crawl.
  bool delete_orphans = 6;
}

// Response from starting a crawl session.
message StartCrawlSessionResponse {
  // Whether session creation succeeded.
  bool success = 1;
  // Server-assigned session identifier.
  string session_id = 2;
  // Client-provided crawl identifier.
  string crawl_id = 3;
  // Status or error message.
  string message = 4;
}

// Request to end an active crawl session.
message EndCrawlSessionRequest {
  // Session identifier to end.
  string session_id = 1;
  // Crawl identifier.
  string crawl_id = 2;
  // Summary statistics for the completed crawl.
  CrawlSummary summary = 3;
}

// Metadata describing a crawl session.
message CrawlMetadata {
  // Type of connector (e.g., "filesystem", "confluence", "database").
  string connector_type = 1;
  // Connector version string.
  string connector_version = 2;
  // Timestamp when crawl started.
  google.protobuf.Timestamp crawl_started = 3;
  // Source system being crawled.
  string source_system = 4;
  // Crawl-specific parameters.
  map<string, string> parameters = 5;
}

// Summary statistics for a completed crawl.
message CrawlSummary {
  // Total documents found during crawl.
  int32 documents_found = 1;
  // Documents successfully processed.
  int32 documents_processed = 2;
  // Documents that failed processing.
  int32 documents_failed = 3;
  // Documents skipped (e.g., unchanged).
  int32 documents_skipped = 4;
  // Total bytes processed.
  int64 bytes_processed = 5;
  // Timestamp when crawl started.
  google.protobuf.Timestamp started = 6;
  // Timestamp when crawl completed.
  google.protobuf.Timestamp completed = 7;
  // Additional statistics as key-value pairs.
  map<string, string> statistics = 8;
}

// Response from ending a crawl session.
message EndCrawlSessionResponse {
  // Whether session end succeeded.
  bool success = 1;
  // Number of orphaned documents found.
  int32 orphans_found = 2;
  // Number of orphaned documents deleted (if delete_orphans was true).
  int32 orphans_deleted = 3;
  // Status or error message.
  string message = 4;
}

// ============================================
// HEARTBEAT
// ============================================

// Request to maintain an active crawl session.
message HeartbeatRequest {
  // Session identifier.
  string session_id = 1;
  // Crawl identifier.
  string crawl_id = 2;
  // Number of documents currently queued for processing.
  int32 documents_queued = 3;
  // Number of documents currently being processed.
  int32 documents_processing = 4;
  // Additional metrics as key-value pairs.
  map<string, string> metrics = 5;
}

// Response from heartbeat with optional control commands.
message HeartbeatResponse {
  // Whether the session is still valid.
  bool session_valid = 1;
  // Optional control command from server to crawler.
  ControlCommand command = 2;
  // Configuration updates to apply.
  map<string, string> config_updates = 3;
}

// Control commands sent from server to crawler.
enum ControlCommand {
  // No command specified.
  CONTROL_COMMAND_UNSPECIFIED = 0;
  // Pause the crawl temporarily.
  CONTROL_COMMAND_COMMAND_PAUSE = 1;
  // Stop the crawl completely.
  CONTROL_COMMAND_COMMAND_STOP = 2;
  // Slow down the crawl rate.
  CONTROL_COMMAND_COMMAND_THROTTLE = 3;
  // Speed up the crawl rate.
  CONTROL_COMMAND_COMMAND_SPEED_UP = 4;
}

// ============================================
// DATASOURCE (Account + Connector Type Binding)
// Instance per account with own API key and config
// ============================================

// DataSource represents an account's binding to a connector type.
//
// Each account can have one DataSource per connector type. The DataSource has
// its own API key and configuration, allowing per-account customization.
// Storage configuration is handled via Drive references (FilesystemService).
message DataSource {
  // Unique datasource identifier.
  // Deterministic: hash(account_id + connector_id)
  string datasource_id = 1;
  // Account that owns this datasource.
  string account_id = 2;
  // Connector type this datasource uses.
  string connector_id = 3;
  // Human-readable datasource name.
  string name = 4;
  // Generated API key for authentication (hashed in storage, plaintext only on creation).
  string api_key = 5;
  // Drive name for document storage (references Drive entity in FilesystemService).
  // The Drive contains S3 bucket, KMS keys, and credentials via Infisical.
  string drive_name = 6;
  // Metadata applied to all documents from this datasource.
  map<string, string> metadata = 7;
  // Maximum file size allowed in bytes.
  int64 max_file_size = 8;
  // Rate limit for uploads per minute.
  int64 rate_limit_per_minute = 9;
  // Whether this datasource is active.
  bool active = 10;
  // Timestamp when datasource was created.
  google.protobuf.Timestamp created_at = 11;
  // Timestamp when datasource was last updated.
  google.protobuf.Timestamp updated_at = 12;
}

// ============================================
// DATASOURCE ADMIN API MESSAGES
// ============================================

// Request to create a new datasource for an account.
message CreateDataSourceRequest {
  // Account that will own this datasource.
  string account_id = 1;
  // Connector type to bind (e.g., "s3", "file-crawler").
  string connector_id = 2;
  // Human-readable datasource name.
  string name = 3;
  // Drive name for document storage (must exist, created via FilesystemService).
  string drive_name = 4;
  // Metadata to apply to all documents from this datasource.
  map<string, string> metadata = 5;
  // Maximum file size allowed in bytes (0 = no limit).
  int64 max_file_size = 6;
  // Rate limit for uploads per minute (0 = no limit).
  int64 rate_limit_per_minute = 7;
}

// Response from datasource creation.
message CreateDataSourceResponse {
  // Whether creation succeeded.
  bool success = 1;
  // The created datasource (includes generated datasource_id and api_key).
  DataSource datasource = 2;
  // Status or error message.
  string message = 3;
}

// Request to update datasource configuration.
message UpdateDataSourceRequest {
  // DataSource identifier to update.
  string datasource_id = 1;
  // Updated datasource name (optional).
  string name = 2;
  // Updated drive name (optional).
  string drive_name = 3;
  // Updated metadata (optional, replaces existing).
  map<string, string> metadata = 4;
  // Updated max file size (optional).
  int64 max_file_size = 5;
  // Updated rate limit (optional).
  int64 rate_limit_per_minute = 6;
}

// Response from datasource update.
message UpdateDataSourceResponse {
  // Whether update succeeded.
  bool success = 1;
  // Status or error message.
  string message = 2;
  // Updated datasource.
  DataSource datasource = 3;
}

// Request to get datasource details.
message GetDataSourceRequest {
  // DataSource identifier to retrieve.
  string datasource_id = 1;
}

// Response containing datasource details.
message GetDataSourceResponse {
  // The datasource details.
  DataSource datasource = 1;
}

// Request to validate an API key.
message ValidateApiKeyRequest {
  // DataSource identifier.
  string datasource_id = 1;
  // API key to validate.
  string api_key = 2;
}

// Response from API key validation.
message ValidateApiKeyResponse {
  // Whether the API key is valid.
  bool valid = 1;
  // Validation message.
  string message = 2;
  // Lightweight config for runtime use (if valid).
  DataSourceConfig config = 3;
}

// Request to list datasources.
message ListDataSourcesRequest {
  // Optional filter by account ID.
  string account_id = 1;
  // Optional filter by connector type.
  string connector_id = 2;
  // Number of results per page.
  int32 page_size = 3;
  // Pagination token.
  string page_token = 4;
  // Whether to include inactive datasources.
  bool include_inactive = 5;
}

// Response containing datasource list.
message ListDataSourcesResponse {
  // List of datasources.
  repeated DataSource datasources = 1;
  // Token for next page.
  string next_page_token = 2;
  // Total count of datasources.
  int32 total_count = 3;
}

// Request to enable or disable a datasource.
message SetDataSourceStatusRequest {
  // DataSource identifier.
  string datasource_id = 1;
  // Whether datasource should be active.
  bool active = 2;
  // Reason for status change.
  string reason = 3;
}

// Response from status change.
message SetDataSourceStatusResponse {
  // Whether status change succeeded.
  bool success = 1;
  // Status or error message.
  string message = 2;
}

// Request to delete a datasource.
message DeleteDataSourceRequest {
  // DataSource identifier to delete.
  string datasource_id = 1;
  // Whether to permanently delete (true) or soft delete (false).
  bool hard_delete = 2;
}

// Response from datasource deletion.
message DeleteDataSourceResponse {
  // Whether deletion succeeded.
  bool success = 1;
  // Status or error message.
  string message = 2;
  // Number of associated crawl sessions deleted.
  int32 crawl_sessions_deleted = 3;
}

// Request to rotate datasource API key.
message RotateApiKeyRequest {
  // DataSource identifier.
  string datasource_id = 1;
  // Whether to invalidate old key immediately or allow grace period.
  bool invalidate_old_immediately = 2;
}

// Response from API key rotation.
message RotateApiKeyResponse {
  // Whether rotation succeeded.
  bool success = 1;
  // New API key.
  string new_api_key = 2;
  // When the old key will stop working.
  google.protobuf.Timestamp old_key_expires = 3;
  // Status or error message.
  string message = 4;
}

// Request to get crawl history for a datasource.
message GetCrawlHistoryRequest {
  // DataSource identifier.
  string datasource_id = 1;
  // Maximum number of sessions to return.
  int32 limit = 2;
  // Pagination token.
  string page_token = 3;
}

// Response containing crawl history.
message GetCrawlHistoryResponse {
  // List of crawl session summaries.
  repeated CrawlSessionSummary sessions = 1;
  // Token for next page.
  string next_page_token = 2;
}

// ============================================
// CONNECTOR TYPE API MESSAGES (Read-only)
// ============================================

// Request to list available connector types.
message ListConnectorTypesRequest {
  // Optional filter by management type.
  ai.pipestream.connector.v1.ManagementType management_type = 1;
  // Number of results per page.
  int32 page_size = 2;
  // Pagination token.
  string page_token = 3;
}

// Response containing connector type list.
message ListConnectorTypesResponse {
  // List of connector types.
  repeated ai.pipestream.connector.v1.Connector connectors = 1;
  // Token for next page.
  string next_page_token = 2;
  // Total count of connector types.
  int32 total_count = 3;
}

// Request to get connector type details.
message GetConnectorTypeRequest {
  // Connector type identifier.
  string connector_id = 1;
}

// Response containing connector type details.
message GetConnectorTypeResponse {
  // The connector type details.
  ai.pipestream.connector.v1.Connector connector = 1;
}

// Summary of a completed crawl session.
message CrawlSessionSummary {
  // Session identifier.
  string session_id = 1;
  // Crawl identifier.
  string crawl_id = 2;
  // Timestamp when crawl started.
  google.protobuf.Timestamp started_at = 3;
  // Timestamp when crawl completed.
  google.protobuf.Timestamp completed_at = 4;
  // Total documents found.
  int32 documents_found = 5;
  // Documents successfully processed.
  int32 documents_processed = 6;
  // Documents that failed processing.
  int32 documents_failed = 7;
  // Total bytes processed.
  int64 bytes_processed = 8;
  // Status of the crawl (e.g., "RUNNING", "COMPLETED", "FAILED").
  string status = 9;
  // Additional metadata as key-value pairs.
  map<string, string> metadata = 10;
}
