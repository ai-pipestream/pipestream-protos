syntax = "proto3";

package ai.pipestream.connector.intake.v1;

import "ai/pipestream/data/v1/pipeline_core_types.proto";
import "google/protobuf/timestamp.proto";

option java_multiple_files = true;
option java_package = "ai.pipestream.connector.intake.v1";

// ============================================
// CONNECTOR-INTAKE-SERVICE
// Handles authentication, account lookup, metadata enrichment
// Agnostic to connector type (filesystem, database, API, etc.)
// ============================================

// High-throughput document ingestion service for all connector types.
//
// This service provides the main entry point for document ingestion into the AI Pipestream platform.
// It handles authentication, account verification, metadata enrichment, and routing to the repository layer.
// Supports both structured PipeDoc uploads and raw blob uploads with automatic wrapping.
service ConnectorIntakeService {
  // 1. The "Power User" path: Client sends a full PipeDoc
  // Best for internal services, advanced connectors, or "Box.com" style integrations
  // where the client already knows the structure.
  rpc UploadPipeDoc(UploadPipeDocRequest) returns (UploadPipeDocResponse);

  // 2. The "Simple" path: Client sends raw bytes (Blob) + minimal metadata
  // Best for "I just have a file" scenarios. Intake constructs the PipeDoc wrapper.
  rpc UploadBlob(UploadBlobRequest) returns (UploadBlobResponse);

  // Register a new crawl session (Async/Background only)
  rpc StartCrawlSession(StartCrawlSessionRequest) returns (StartCrawlSessionResponse);

  // End a crawl session (for cleanup, orphan detection)
  rpc EndCrawlSession(EndCrawlSessionRequest) returns (EndCrawlSessionResponse);

  // Heartbeat to keep session alive
  rpc Heartbeat(HeartbeatRequest) returns (HeartbeatResponse);
}

// ============================================
// DATASOURCE ADMIN SERVICE
// Manages DataSources (account + connector type bindings)
// ============================================

// Administrative operations for datasource lifecycle management.
//
// A DataSource represents an account's binding to a connector type with its own API key.
// Connector types are pre-seeded templates (e.g., "s3", "file-crawler").
// DataSource = Account + Connector Type + API Key + Configuration.
service DataSourceAdminService {
  // Create a new datasource for an account
  rpc CreateDataSource(CreateDataSourceRequest) returns (CreateDataSourceResponse);

  // Update datasource configuration
  rpc UpdateDataSource(UpdateDataSourceRequest) returns (UpdateDataSourceResponse);

  // Get datasource details
  rpc GetDataSource(GetDataSourceRequest) returns (GetDataSourceResponse);

  // Validate API key for a datasource
  rpc ValidateApiKey(ValidateApiKeyRequest) returns (ValidateApiKeyResponse);

  // List all datasources for an account
  rpc ListDataSources(ListDataSourcesRequest) returns (ListDataSourcesResponse);

  // Enable/disable a datasource
  rpc SetDataSourceStatus(SetDataSourceStatusRequest) returns (SetDataSourceStatusResponse);

  // Delete a datasource (soft delete)
  rpc DeleteDataSource(DeleteDataSourceRequest) returns (DeleteDataSourceResponse);

  // Generate new API key for a datasource
  rpc RotateApiKey(RotateApiKeyRequest) returns (RotateApiKeyResponse);

  // Get crawl history for a datasource
  rpc GetCrawlHistory(GetCrawlHistoryRequest) returns (GetCrawlHistoryResponse);

  // List available connector types (pre-seeded templates)
  rpc ListConnectorTypes(ListConnectorTypesRequest) returns (ListConnectorTypesResponse);

  // Get connector type details
  rpc GetConnectorType(GetConnectorTypeRequest) returns (GetConnectorTypeResponse);
}

// ============================================
// UPLOAD MESSAGES
// ============================================

// Request to upload a fully-formed PipeDoc.
//
// Power user path for internal services and advanced connectors.
message UploadPipeDocRequest {
  // DataSource identifier (deterministic: hash of account_id + connector_id).
  string datasource_id = 1;
  // API key or JWT token for authentication.
  string api_key = 2;
  // Optional session ID for grouping related uploads.
  string session_id = 3;
  // The full document structure with all metadata and content.
  ai.pipestream.data.v1.PipeDoc pipe_doc = 4;
  // Source document identifier from the origin system.
  // Used to generate deterministic doc_id: hash(datasource_id + source_doc_id)
  string source_doc_id = 5;
}

// Request to upload raw file content with minimal metadata.
//
// Simple path for basic file uploads where the service constructs the PipeDoc wrapper.
message UploadBlobRequest {
  // DataSource identifier (deterministic: hash of account_id + connector_id).
  string datasource_id = 1;
  // API key for authentication.
  string api_key = 2;
  // Optional session ID for grouping related uploads.
  string session_id = 3;
  // Original filename.
  string filename = 4;
  // MIME type of the content.
  string mime_type = 5;
  // Virtual path in the datasource's namespace (e.g., "/folder/file.txt").
  string path = 6;
  // Additional metadata key-value pairs.
  map<string, string> metadata = 7;
  // Raw file content (up to 2GB).
  bytes content = 8;
  // Source document identifier from the origin system.
  // Used to generate deterministic doc_id: hash(datasource_id + source_doc_id)
  // If not provided, path or filename will be used.
  string source_doc_id = 9;
}

// Response from document upload operations.
message UploadPipeDocResponse {
  // Whether the upload succeeded.
  bool success = 1;
  // Document ID assigned by the repository service.
  string doc_id = 2;
  // Status or error message.
  string message = 3;
}

message UploadBlobResponse {
  // Whether the upload succeeded.
  bool success = 1;
  // Document ID assigned by the repository service.
  string doc_id = 2;
  // Status or error message.
  string message = 3;
}

// ============================================
// DATASOURCE CONFIG
// Simplified config returned after validation
// ============================================

// DataSource configuration returned after authentication.
// This is a lightweight view for runtime use (no secrets, no full Drive details).
message DataSourceConfig {
  // Account owning this datasource.
  string account_id = 1;
  // DataSource identifier (deterministic).
  string datasource_id = 2;
  // Connector type (e.g., "s3", "file-crawler").
  string connector_id = 3;
  // Drive name for storage (resolve via FilesystemService for S3 details).
  string drive_name = 4;
  // Maximum file size allowed in bytes.
  int64 max_file_size = 5;
  // Rate limit for uploads per minute.
  int64 rate_limit_per_minute = 6;
  // Metadata applied to all documents from this datasource.
  map<string, string> metadata = 7;
}

// ============================================
// CRAWL SESSION MANAGEMENT
// ============================================

// Request to start a new crawl session.
message StartCrawlSessionRequest {
  // DataSource identifier.
  string datasource_id = 1;
  // API key for authentication.
  string api_key = 2;
  // Client-provided crawl identifier.
  string crawl_id = 3;
  // Crawl metadata and parameters.
  CrawlMetadata metadata = 4;
  // Whether to track all documents in this crawl for orphan detection.
  bool track_documents = 5;
  // Whether to delete documents not seen in this crawl.
  bool delete_orphans = 6;
}

// Response from starting a crawl session.
message StartCrawlSessionResponse {
  // Whether session creation succeeded.
  bool success = 1;
  // Server-assigned session identifier.
  string session_id = 2;
  // Client-provided crawl identifier.
  string crawl_id = 3;
  // Status or error message.
  string message = 4;
}

// Request to end an active crawl session.
message EndCrawlSessionRequest {
  // Session identifier to end.
  string session_id = 1;
  // Crawl identifier.
  string crawl_id = 2;
  // Summary statistics for the completed crawl.
  CrawlSummary summary = 3;
}

// Metadata describing a crawl session.
message CrawlMetadata {
  // Type of connector (e.g., "filesystem", "confluence", "database").
  string connector_type = 1;
  // Connector version string.
  string connector_version = 2;
  // Timestamp when crawl started.
  google.protobuf.Timestamp crawl_started = 3;
  // Source system being crawled.
  string source_system = 4;
  // Crawl-specific parameters.
  map<string, string> parameters = 5;
}

// Summary statistics for a completed crawl.
message CrawlSummary {
  // Total documents found during crawl.
  int32 documents_found = 1;
  // Documents successfully processed.
  int32 documents_processed = 2;
  // Documents that failed processing.
  int32 documents_failed = 3;
  // Documents skipped (e.g., unchanged).
  int32 documents_skipped = 4;
  // Total bytes processed.
  int64 bytes_processed = 5;
  // Timestamp when crawl started.
  google.protobuf.Timestamp started = 6;
  // Timestamp when crawl completed.
  google.protobuf.Timestamp completed = 7;
  // Additional statistics as key-value pairs.
  map<string, string> statistics = 8;
}

// Response from ending a crawl session.
message EndCrawlSessionResponse {
  // Whether session end succeeded.
  bool success = 1;
  // Number of orphaned documents found.
  int32 orphans_found = 2;
  // Number of orphaned documents deleted (if delete_orphans was true).
  int32 orphans_deleted = 3;
  // Status or error message.
  string message = 4;
}

// ============================================
// HEARTBEAT
// ============================================

// Request to maintain an active crawl session.
message HeartbeatRequest {
  // Session identifier.
  string session_id = 1;
  // Crawl identifier.
  string crawl_id = 2;
  // Number of documents currently queued for processing.
  int32 documents_queued = 3;
  // Number of documents currently being processed.
  int32 documents_processing = 4;
  // Additional metrics as key-value pairs.
  map<string, string> metrics = 5;
}

// Response from heartbeat with optional control commands.
message HeartbeatResponse {
  // Whether the session is still valid.
  bool session_valid = 1;
  // Optional control command from server to crawler.
  ControlCommand command = 2;
  // Configuration updates to apply.
  map<string, string> config_updates = 3;
}

// Control commands sent from server to crawler.
enum ControlCommand {
  // No command specified.
  CONTROL_COMMAND_UNSPECIFIED = 0;
  // Pause the crawl temporarily.
  CONTROL_COMMAND_COMMAND_PAUSE = 1;
  // Stop the crawl completely.
  CONTROL_COMMAND_COMMAND_STOP = 2;
  // Slow down the crawl rate.
  CONTROL_COMMAND_COMMAND_THROTTLE = 3;
  // Speed up the crawl rate.
  CONTROL_COMMAND_COMMAND_SPEED_UP = 4;
}

// ============================================
// CONNECTOR TYPE (Template/Registry)
// Pre-seeded connector types like "s3", "file-crawler"
// ============================================

// Management type for connectors.
enum ManagementType {
  // Unspecified management type.
  MANAGEMENT_TYPE_UNSPECIFIED = 0;
  // Unmanaged: External systems push documents via API key. No connector app.
  MANAGEMENT_TYPE_UNMANAGED = 1;
  // Managed: Platform-managed connector with health checks, scheduling, etc.
  MANAGEMENT_TYPE_MANAGED = 2;
}

// Connector type definition (template/registry).
//
// Connectors are pre-seeded types that define what kind of data source integration
// is being used (e.g., "s3", "file-crawler", "confluence"). They are reusable across
// accounts - each account creates a DataSource binding to use a connector type.
message Connector {
  // Unique connector type identifier.
  // Deterministic: hash(connector_type)
  string connector_id = 1;
  // Connector type name (e.g., "s3", "file-crawler", "confluence").
  string connector_type = 2;
  // Human-readable display name.
  string name = 3;
  // Description of the connector type.
  string description = 4;
  // Whether this is managed or unmanaged.
  ManagementType management_type = 5;
  // Timestamp when connector type was created.
  google.protobuf.Timestamp created_at = 6;
  // Timestamp when connector type was last updated.
  google.protobuf.Timestamp updated_at = 7;
}

// ============================================
// DATASOURCE (Account + Connector Type Binding)
// Instance per account with own API key and config
// ============================================

// DataSource represents an account's binding to a connector type.
//
// Each account can have one DataSource per connector type. The DataSource has
// its own API key and configuration, allowing per-account customization.
// Storage configuration is handled via Drive references (FilesystemService).
message DataSource {
  // Unique datasource identifier.
  // Deterministic: hash(account_id + connector_id)
  string datasource_id = 1;
  // Account that owns this datasource.
  string account_id = 2;
  // Connector type this datasource uses.
  string connector_id = 3;
  // Human-readable datasource name.
  string name = 4;
  // Generated API key for authentication (hashed in storage, plaintext only on creation).
  string api_key = 5;
  // Drive name for document storage (references Drive entity in FilesystemService).
  // The Drive contains S3 bucket, KMS keys, and credentials via Infisical.
  string drive_name = 6;
  // Metadata applied to all documents from this datasource.
  map<string, string> metadata = 7;
  // Maximum file size allowed in bytes.
  int64 max_file_size = 8;
  // Rate limit for uploads per minute.
  int64 rate_limit_per_minute = 9;
  // Whether this datasource is active.
  bool active = 10;
  // Timestamp when datasource was created.
  google.protobuf.Timestamp created_at = 11;
  // Timestamp when datasource was last updated.
  google.protobuf.Timestamp updated_at = 12;
}

// ============================================
// DATASOURCE ADMIN API MESSAGES
// ============================================

// Request to create a new datasource for an account.
message CreateDataSourceRequest {
  // Account that will own this datasource.
  string account_id = 1;
  // Connector type to bind (e.g., "s3", "file-crawler").
  string connector_id = 2;
  // Human-readable datasource name.
  string name = 3;
  // Drive name for document storage (must exist, created via FilesystemService).
  string drive_name = 4;
  // Metadata to apply to all documents from this datasource.
  map<string, string> metadata = 5;
  // Maximum file size allowed in bytes (0 = no limit).
  int64 max_file_size = 6;
  // Rate limit for uploads per minute (0 = no limit).
  int64 rate_limit_per_minute = 7;
}

// Response from datasource creation.
message CreateDataSourceResponse {
  // Whether creation succeeded.
  bool success = 1;
  // The created datasource (includes generated datasource_id and api_key).
  DataSource datasource = 2;
  // Status or error message.
  string message = 3;
}

// Request to update datasource configuration.
message UpdateDataSourceRequest {
  // DataSource identifier to update.
  string datasource_id = 1;
  // Updated datasource name (optional).
  string name = 2;
  // Updated drive name (optional).
  string drive_name = 3;
  // Updated metadata (optional, replaces existing).
  map<string, string> metadata = 4;
  // Updated max file size (optional).
  int64 max_file_size = 5;
  // Updated rate limit (optional).
  int64 rate_limit_per_minute = 6;
}

// Response from datasource update.
message UpdateDataSourceResponse {
  // Whether update succeeded.
  bool success = 1;
  // Status or error message.
  string message = 2;
  // Updated datasource.
  DataSource datasource = 3;
}

// Request to get datasource details.
message GetDataSourceRequest {
  // DataSource identifier to retrieve.
  string datasource_id = 1;
}

// Response containing datasource details.
message GetDataSourceResponse {
  // The datasource details.
  DataSource datasource = 1;
}

// Request to validate an API key.
message ValidateApiKeyRequest {
  // DataSource identifier.
  string datasource_id = 1;
  // API key to validate.
  string api_key = 2;
}

// Response from API key validation.
message ValidateApiKeyResponse {
  // Whether the API key is valid.
  bool valid = 1;
  // Validation message.
  string message = 2;
  // Lightweight config for runtime use (if valid).
  DataSourceConfig config = 3;
}

// Request to list datasources.
message ListDataSourcesRequest {
  // Optional filter by account ID.
  string account_id = 1;
  // Optional filter by connector type.
  string connector_id = 2;
  // Number of results per page.
  int32 page_size = 3;
  // Pagination token.
  string page_token = 4;
  // Whether to include inactive datasources.
  bool include_inactive = 5;
}

// Response containing datasource list.
message ListDataSourcesResponse {
  // List of datasources.
  repeated DataSource datasources = 1;
  // Token for next page.
  string next_page_token = 2;
  // Total count of datasources.
  int32 total_count = 3;
}

// Request to enable or disable a datasource.
message SetDataSourceStatusRequest {
  // DataSource identifier.
  string datasource_id = 1;
  // Whether datasource should be active.
  bool active = 2;
  // Reason for status change.
  string reason = 3;
}

// Response from status change.
message SetDataSourceStatusResponse {
  // Whether status change succeeded.
  bool success = 1;
  // Status or error message.
  string message = 2;
}

// Request to delete a datasource.
message DeleteDataSourceRequest {
  // DataSource identifier to delete.
  string datasource_id = 1;
  // Whether to permanently delete (true) or soft delete (false).
  bool hard_delete = 2;
}

// Response from datasource deletion.
message DeleteDataSourceResponse {
  // Whether deletion succeeded.
  bool success = 1;
  // Status or error message.
  string message = 2;
  // Number of associated crawl sessions deleted.
  int32 crawl_sessions_deleted = 3;
}

// Request to rotate datasource API key.
message RotateApiKeyRequest {
  // DataSource identifier.
  string datasource_id = 1;
  // Whether to invalidate old key immediately or allow grace period.
  bool invalidate_old_immediately = 2;
}

// Response from API key rotation.
message RotateApiKeyResponse {
  // Whether rotation succeeded.
  bool success = 1;
  // New API key.
  string new_api_key = 2;
  // When the old key will stop working.
  google.protobuf.Timestamp old_key_expires = 3;
  // Status or error message.
  string message = 4;
}

// Request to get crawl history for a datasource.
message GetCrawlHistoryRequest {
  // DataSource identifier.
  string datasource_id = 1;
  // Maximum number of sessions to return.
  int32 limit = 2;
  // Pagination token.
  string page_token = 3;
}

// Response containing crawl history.
message GetCrawlHistoryResponse {
  // List of crawl session summaries.
  repeated CrawlSessionSummary sessions = 1;
  // Token for next page.
  string next_page_token = 2;
}

// ============================================
// CONNECTOR TYPE API MESSAGES (Read-only)
// ============================================

// Request to list available connector types.
message ListConnectorTypesRequest {
  // Optional filter by management type.
  ManagementType management_type = 1;
  // Number of results per page.
  int32 page_size = 2;
  // Pagination token.
  string page_token = 3;
}

// Response containing connector type list.
message ListConnectorTypesResponse {
  // List of connector types.
  repeated Connector connectors = 1;
  // Token for next page.
  string next_page_token = 2;
  // Total count of connector types.
  int32 total_count = 3;
}

// Request to get connector type details.
message GetConnectorTypeRequest {
  // Connector type identifier.
  string connector_id = 1;
}

// Response containing connector type details.
message GetConnectorTypeResponse {
  // The connector type details.
  Connector connector = 1;
}

// Summary of a completed crawl session.
message CrawlSessionSummary {
  // Session identifier.
  string session_id = 1;
  // Crawl identifier.
  string crawl_id = 2;
  // Timestamp when crawl started.
  google.protobuf.Timestamp started_at = 3;
  // Timestamp when crawl completed.
  google.protobuf.Timestamp completed_at = 4;
  // Total documents found.
  int32 documents_found = 5;
  // Documents successfully processed.
  int32 documents_processed = 6;
  // Documents that failed processing.
  int32 documents_failed = 7;
  // Total bytes processed.
  int64 bytes_processed = 8;
  // Status of the crawl (e.g., "RUNNING", "COMPLETED", "FAILED").
  string status = 9;
  // Additional metadata as key-value pairs.
  map<string, string> metadata = 10;
}
