syntax = "proto3";

package ai.pipestream.data.v1;

import "google/protobuf/any.proto";
import "google/protobuf/struct.proto";
import "google/protobuf/timestamp.proto";

option java_multiple_files = true;
option java_outer_classname = "PipelineCoreTypesV1Proto";
option java_package = "ai.pipestream.data.v1";

// Document ID derivation method and details.
//
// Records how the doc_id was determined for auditability, debugging,
// and understanding idempotency semantics.
enum DocIdDerivationMethod {
  DOC_ID_DERIVATION_METHOD_UNSPECIFIED = 0;

  // doc_id was already set by the client in PipeDoc.
  DOC_ID_DERIVATION_METHOD_CLIENT_PROVIDED = 1;

  // doc_id derived from intake request source_doc_id.
  DOC_ID_DERIVATION_METHOD_SOURCE_DOC_ID = 2;

  // doc_id derived from PipeDoc.search_metadata.source_uri (canonicalized).
  DOC_ID_DERIVATION_METHOD_SOURCE_URI = 3;

  // doc_id derived from PipeDoc.search_metadata.source_path (normalized).
  DOC_ID_DERIVATION_METHOD_SOURCE_PATH = 4;

  // doc_id was generated by the server (explicit opt-in only).
  DOC_ID_DERIVATION_METHOD_SERVER_GENERATED = 5;
}

message DocIdDerivation {
  DocIdDerivationMethod method = 1;

  // The raw value used to derive the id (e.g., source_doc_id, source_uri, source_path).
  // If sensitive, this can be redacted by the caller, but the field is available.
  string input = 2;

  // The canonical/normalized value actually hashed (optional but very useful for debugging).
  string canonical_input = 3;
}

// Ownership context for a document.
//
// Tracks which account and datasource created or owns this document.
// Essential for security auditing, access control, and traceability.
message OwnershipContext {
  // Account that owns this document.
  string account_id = 1;
  // DataSource (account + connector type binding) that produced this document.
  // Deterministic: hash(account_id + connector_id)
  string datasource_id = 2;
  // Connector type that created this document (e.g., "s3", "file-crawler").
  // Optional since it's derivable from datasource, but useful for filtering.
  optional string connector_id = 3;
}

// Core document container for the pipeline processing system.
//
// PipeDoc is the primary data structure flowing through the pipeline. It contains standardized
// search metadata, binary content (blobs), and flexible structured data for custom use cases.
message PipeDoc {
  // Unique identifier for this document across the entire system.
  // Deterministic: hash(datasource_id + source_doc_id)
  string doc_id = 1;
  // Standardized search engine metadata for OOTB indexing and querying.
  //
  // This is intentionally less flexible to maintain a common API layer that all
  // out-of-the-box indexing steps can rely on.
  SearchMetadata search_metadata = 2;
  // Binary content container for document parsing.
  //
  // Useful for storing files that need parsing (images, PDFs, videos, etc.).
  BlobBag blob_bag = 3;
  // Customer-provided structured data in any protobuf format.
  //
  // Types registered in the schema registry with JARs in the classpath will deserialize
  // to their actual type. Unknown types fall back to dynamic messages. All data here
  // will be indexed into the search engine unless configured otherwise.
  google.protobuf.Any structured_data = 4;
  // Immutable parsed metadata from document parsers (Tika, Docling, etc.).
  //
  // This field is append-only - parsers can add new entries but should never modify
  // or delete existing ones. Keys are parser-specific identifiers (e.g., "tika", "docling").
  // The actual parsed data is stored in ParsedMetadata.data as an Any type containing
  // parser-specific protobuf messages (e.g., TikaResponse, DoclingResponse).
  map<string, ParsedMetadata> parsed_metadata = 5;
  // Ownership tracking for security, auditing, and access control.
  //
  // Captures which account and datasource produced this document.
  optional OwnershipContext ownership = 6;
  // How the doc_id was determined (for auditability, debugging, and idempotency semantics).
  //
  // Records the derivation method and input used to create the doc_id.
  // Useful for understanding replacement semantics and debugging doc_id collisions.
  optional DocIdDerivation doc_id_derivation = 7;
}

// Wrapper for parser output metadata.
//
// Contains metadata about a parser's execution and the actual parsed data.
// The data field uses google.protobuf.Any to support any parser-specific protobuf type.
message ParsedMetadata {
  // Name of the parser that produced this data (e.g., "tika", "docling").
  string parser_name = 1;
  // Version of the parser (e.g., "4.0.0", "1.2.3").
  string parser_version = 2;
  // Timestamp when parsing occurred.
  google.protobuf.Timestamp parsed_at = 3;
  // Parser-specific output data.
  //
  // Contains parser-specific protobuf messages such as TikaResponse, DoclingResponse, etc.
  // Types registered in the schema registry will deserialize to their actual type.
  google.protobuf.Any data = 4;
}

// Standardized search engine metadata for OOTB indexing.
//
// Provides a common data layout for search API queries. This structure maintains
// consistency across different document types while supporting enhanced search features
// like semantic chunking, document outlines, and link extraction.
message SearchMetadata {
  // Document title or heading.
  optional string title = 1;
  // Full text content of the document.
  optional string body = 2;
  // Extracted keywords for search and categorization.
  optional Keywords keywords = 3;
  // Document type classification (e.g., "PDF", "HTML", "DOCX").
  optional string document_type = 4;
  // Original source URI where the document was obtained.
  optional string source_uri = 5;
  // MIME type of the source document.
  optional string source_mime_type = 6;
  // Timestamp when the document was originally created.
  optional google.protobuf.Timestamp creation_date = 7;
  // Timestamp when the document was last modified at source.
  optional google.protobuf.Timestamp last_modified_date = 8;
  // Timestamp when the document was processed by the pipeline.
  optional google.protobuf.Timestamp processed_date = 9;
  // Language code of the document content (e.g., "en", "es", "fr").
  optional string language = 10;
  // Document author or creator.
  optional string author = 11;
  // Category or classification label for the document.
  optional string category = 12;
  // Key-value tags for flexible categorization.
  optional Tags tags = 13;
  // Length of the document content in bytes or characters.
  optional int32 content_length = 14;
  // Relevance score for search ranking purposes.
  optional double relevance_score = 15;
  // Additional custom fields not covered by standard schema.
  optional google.protobuf.Struct custom_fields = 16;
  // Generic key-value metadata for extensibility.
  map<string, string> metadata = 17;
  // Results from semantic chunking and embedding processes.
  //
  // Supports multiple processing configurations applied to the same document.
  repeated SemanticProcessingResult semantic_results = 18;
  // Digital Object Identifier for academic papers and publications.
  optional string doi = 19;
  // Neutral document outline for navigation and chunking.
  //
  // Supports EPUB TOC, HTML headings, Markdown headings, LaTeX sections, PDF outlines, etc.
  optional DocOutline doc_outline = 20;
  // Links discovered within the document.
  //
  // Useful for analyzing document relationships and external references.
  repeated LinkReference discovered_links = 21;
  // Path portion of source_uri for categorization.
  //
  // Example: "/docs/legal/NY/file.pdf" from "https://example.com/docs/legal/NY/file.pdf"
  optional string source_path = 22;
  // Split path segments from source_path.
  //
  // Example: ["docs", "legal", "NY", "file.pdf"]
  repeated string source_path_segments = 23;
  // Last segment of source_path.
  //
  // Example: "file.pdf" from "/docs/legal/NY/file.pdf"
  optional string source_slug = 24;
}

// Collection of keywords extracted from a document.
message Keywords {
  // List of keyword strings for search and categorization.
  repeated string keyword = 1;
}

// Key-value tag collection for flexible document categorization.
message Tags {
  // Tag data as key-value pairs for custom metadata.
  map<string, string> tag_data = 1;
}

// Format-neutral document outline representation.
//
// Populated from various sources: EPUB nav/TOC, HTML headings, Markdown headings,
// LaTeX sectioning, PDF outlines, etc. Provides consistent structure for navigation.
message DocOutline {
  // Sections in preorder traversal order.
  repeated Section sections = 1;
}

// Single section within a document outline.
//
// Represents a heading, chapter, section, or other structural element.
message Section {
  // Stable identifier if present in source document.
  optional string id = 1;
  // Section title or heading text.
  optional string title = 2;
  // Nesting depth where 0 is top level.
  int32 depth = 3;
  // Heading level (HTML h1=1, Markdown #=1, 0 if unknown).
  int32 heading_level = 4;
  // Relative link or fragment identifier if applicable.
  optional string href = 5;
  // Parent section identifier for hierarchical navigation.
  optional string parent_id = 6;
  // Preorder position index within the document.
  int32 order_index = 7;
  // Classification tags (e.g., "chapter", "appendix", "figure", "table", "nav").
  repeated string tags = 8;
  // Relative font size (0.0-1.0) if inferred from styling.
  optional float font_size_rel = 9;
  // Starting page number if known (PDF/EPUB).
  optional int32 page_start = 10;
  // Ending page number if known (PDF/EPUB).
  optional int32 page_end = 11;
}

// Reference to a link discovered within a document.
//
// Used for analyzing document relationships and external references.
message LinkReference {
  // Href or absolute URL when resolvable.
  string url = 1;
  // Anchor text or link label.
  optional string text = 2;
  // rel attribute if present (e.g., "nofollow", "canonical").
  optional string rel = 3;
  // MIME type attribute or inferred type.
  optional string type = 4;
  // Whether the link points to a different host than source_uri.
  optional bool is_external = 5;
}

// Container for one or more binary blobs.
//
// Supports either a single blob or multiple blobs for flexible content storage.
message BlobBag {
  // Either a single blob or multiple blobs.
  oneof blob_data {
    // Single blob content.
    Blob blob = 1;
    // Multiple blob contents.
    Blobs blobs = 2;
  }
}

// Collection of multiple blobs.
message Blobs {
  // List of blob objects.
  repeated Blob blob = 1;
}

// Reference to a file stored in S3 or external storage.
//
// Maps to the virtual filesystem's drive and object key structure.
message FileStorageReference {
  // Drive name mapping to S3 bucket name.
  string drive_name = 1;
  // Object path/key within the bucket.
  string object_key = 2;
  // S3 version ID if versioning is enabled.
  optional string version_id = 3;
}

// Binary blob with flexible storage options.
//
// Supports both inline data storage (for small files or sensitive data) and
// external S3 storage references. Metadata is always stored in the database.
message Blob {
  // Unique identifier for this blob.
  string blob_id = 1;
  // Drive/bucket identifier where this blob belongs.
  string drive_id = 2;
  // Blob content stored either inline or externally.
  oneof content {
    // Inline binary data for small files or sensitive content.
    bytes data = 3;
    // Reference to external S3 storage for large files.
    FileStorageReference storage_ref = 4;
  }
  // MIME type of the blob content.
  optional string mime_type = 5;
  // Original filename if available.
  optional string filename = 6;
  // Character encoding for text-based blobs.
  optional string encoding = 7;
  // Size of the blob content in bytes.
  int64 size_bytes = 8;
  // Checksum value for integrity verification.
  optional string checksum = 9;
  // Type of checksum algorithm used.
  ChecksumType checksum_type = 10;
  // Additional blob metadata stored in the database.
  optional google.protobuf.Struct metadata = 11;
}

// Checksum algorithm types for blob integrity verification.
enum ChecksumType {
  // Unspecified checksum type.
  CHECKSUM_TYPE_UNSPECIFIED = 0;
  // MD5 checksum.
  CHECKSUM_TYPE_MD5 = 1;
  // SHA-1 checksum.
  CHECKSUM_TYPE_SHA1 = 2;
  // SHA-256 checksum.
  CHECKSUM_TYPE_SHA256 = 3;
  // SHA-512 checksum.
  CHECKSUM_TYPE_SHA512 = 4;
}

// ============================================
// INGESTION CONFIGURATION TYPES
// Used by connector-intake-service and engine for per-stream config
// ============================================

// Mode indicating how a document entered the ingestion pipeline.
enum IngressMode {
  // Unspecified ingress mode.
  INGRESS_MODE_UNSPECIFIED = 0;
  // Document was persisted to S3 via HTTP upload or gRPC with persistence enabled.
  INGRESS_MODE_HTTP_STAGED = 1;
  // Document was passed inline via gRPC without persistence.
  INGRESS_MODE_GRPC_INLINE = 2;
}

// Configuration for hydration/dehydration policies for blob handling.
//
// Controls how blobs are handled during pipeline processing - whether to keep
// them inline, use storage references, or hydrate on-demand.
message HydrationConfig {
  // Default hydration policy for processing nodes.
  HydrationPolicy default_hydration_policy = 1;

  // Blob handling after parsing.
  bool drop_blobs_after_parse = 2; // Remove inline blobs after parsing (keep storage_ref).
  bool keep_blobs_as_ref = 3; // Keep original blob reference even after parsing.

  // Inline blob size limits.
  int32 max_inline_blob_size_bytes = 4; // Max size for inline blobs (larger â†’ storage_ref).

  // Hydration behavior for multi-hop processing.
  bool allow_inline_blobs_for_parser_chains = 5; // Allow inline blobs between parsers.

  // Hydration policy options.
  enum HydrationPolicy {
    HYDRATION_POLICY_UNSPECIFIED = 0;
    HYDRATION_POLICY_ALWAYS_INLINE = 1; // Always keep blobs inline (for small docs).
    HYDRATION_POLICY_ALWAYS_REF = 2; // Always use storage_ref (for large docs).
    HYDRATION_POLICY_AUTO = 3; // Auto-select based on size.
    HYDRATION_POLICY_ON_DEMAND = 4; // Hydrate only when needed by module.
  }
}

// Configuration for document persistence decisions.
//
// Controls whether documents are persisted to storage and under what conditions.
// Used by both datasource-admin (Tier 1) and engine (Tier 2) configurations.
message PersistenceConfig {
  // For HTTP uploads: always persisted (streaming requirement).
  // For gRPC PipeDoc uploads: configurable.
  // Default: true (safe default - always persist unless explicitly disabled).
  bool persist_pipedoc = 1;

  // Maximum size (in bytes) for inline documents before forcing persistence.
  // Documents larger than this are always persisted to storage.
  int32 max_inline_size_bytes = 2;
}

// Configuration for document retention policies.
//
// Controls how long documents are retained in storage and how many versions to keep.
// Used by both datasource-admin (Tier 1) and engine (Tier 2) configurations.
message RetentionConfig {
  // Retention policy type.
  RetentionPolicy policy = 1;

  // Retention period in days.
  // -1 = indefinite, 0 = no retention, >0 = number of days.
  int64 retention_days = 2;

  // Maximum versions to keep (if policy is VERSION_BASED).
  int32 max_versions = 3;

  // Retention policy options.
  enum RetentionPolicy {
    RETENTION_POLICY_UNSPECIFIED = 0;
    RETENTION_POLICY_INDEFINITE = 1; // Keep forever.
    RETENTION_POLICY_TIME_BASED = 2; // Delete after retention_days.
    RETENTION_POLICY_VERSION_BASED = 3; // Keep N versions.
    RETENTION_POLICY_NO_RETENTION = 4; // Ephemeral (no archival).
  }
}

// Output hints for downstream processing and routing.
//
// Contains routing information and hints for how the document should be
// processed by downstream modules and where it should ultimately be stored/indexed.
message OutputHints {
  // OpenSearch collection/index hint.
  //
  // Specifies the target collection or index where this document should be indexed.
  string desired_collection = 1;

  // Routing hints for downstream modules.
  //
  // Key-value pairs that modules can use to make routing or processing decisions.
  map<string, string> routing_hints = 2;
}

// Ingestion configuration from connector-intake-service.
//
// Contains the resolved 2-tier configuration (Tier 1 + Tier 2 merged) that
// controls how a document is processed through the pipeline. Built by
// connector-intake-service and passed to the engine via StreamMetadata.
message IngestionConfig {
  // How the document entered the pipeline.
  IngressMode ingress_mode = 1;

  // Hydration policy for blob handling during processing.
  //
  // Resolved from Tier 1 (DataSource) + Tier 2 (DatasourceInstance) config.
  HydrationConfig hydration_config = 2;

  // Routing hints for downstream processing.
  //
  // From Tier 2 (DatasourceInstance) config - specifies target collection
  // and other routing hints for modules.
  OutputHints output_hints = 3;

  // Connector-specific custom configuration.
  //
  // Merged from Tier 1 + Tier 2 custom_config fields. This is the only
  // configuration that uses JSON Schema validation (connector-specific).
  google.protobuf.Struct custom_config = 4;
}

// Metadata for tracking stream processing history and context.
//
// Contains execution records, tracing information, error tracking, and datasource
// context for a stream.
message StreamMetadata {
  // Source identifier for validation and tracking.
  string source_id = 2;
  // Timestamp when the stream was created.
  google.protobuf.Timestamp created_at = 3;
  // Timestamp when the stream was last processed.
  google.protobuf.Timestamp last_processed_at = 4;
  // Distributed tracing identifier.
  string trace_id = 5;
  // Complete execution history of all processing steps.
  repeated StepExecutionRecord history = 6;
  // Critical stream-level errors if any occurred.
  optional ErrorData stream_error = 7;
  // Stream-level context parameters for processing.
  map<string, string> context_params = 8;
  // DataSource identifier for tracking document origin.
  //
  // Deterministically derived from account_id and connector type.
  // Used to route documents to the correct pipeline graph entry point.
  string datasource_id = 9;
  // Account identifier for ownership and access control.
  //
  // Links this stream to a specific tenant/customer account.
  string account_id = 10;
  // Entry node identifier where this stream entered the pipeline.
  //
  // Useful for tracking which connector/datasource initiated processing
  // and for routing decisions in multi-entry-point graphs.
  string entry_node_id = 11;
  // Ingestion configuration from connector-intake-service.
  //
  // Contains the resolved 2-tier configuration (Tier 1 + Tier 2 merged)
  // for hydration, routing, and custom connector settings.
  // Only present for streams that entered via intake connectors.
  optional IngestionConfig ingestion_config = 12;
}

// Reference to a document stored in the repository (S3 offload).
//
// Used when the full PipeDoc is stored in S3/repository and only a reference
// is needed in the stream for efficient message passing.
// Engine uses this to hydrate the document via repo-service.
message DocumentReference {
  // Document identifier.
  //
  // The logical document identifier that persists across all versions/hops.
  // Deterministic: hash(datasource_id + source_doc_id)
  string doc_id = 1;
  // Node that produced this version (determines storage location).
  //
  // Identifies exactly which node produced the version of the document being referenced.
  // This is critical for fetching the correct state from the Repository.
  string source_node_id = 2;
  // Account context for multi-tenant lookup.
  //
  // Ensures that hydration requests are scoped to the correct tenant,
  // preventing cross-account data access.
  string account_id = 3;
}

// TCP-like packet header for node-to-node document processing.
//
// Contains the document payload, routing information, processing history,
// and configuration for the current processing step.
message PipeStream {
  // Unique identifier for this processing stream.
  string stream_id = 1;
  // Stream metadata including history and tracing.
  StreamMetadata metadata = 2;
  // Document payload - either inline or stored in repository.
  //
  // For small documents or when full content is needed, use document.
  // For large documents or when offloaded to S3, use document_ref.
  oneof document_payload {
    // Inline document (typically for gRPC direct path or small documents).
    PipeDoc document = 3;
    // Reference to document stored in repository (typically for Kafka path or large documents).
    DocumentReference document_ref = 9;
  }
  // Cluster where processing is occurring.
  string cluster_id = 4;
  // Current node processing this stream.
  string current_node_id = 5;
  // Number of processing hops taken.
  int64 hop_count = 6;
  // Ordered list of node IDs traversed during processing.
  repeated string processing_path = 7;
  // Configuration for the current node's processing step.
  NodeProcessingConfig node_processing_config = 8;
}

// Processing intent for a document.
enum Intent {
  // Unspecified intent.
  INTENT_UNSPECIFIED = 0;
  // Add new document to the index.
  INTENT_ADD = 1;
  // Update existing document in the index.
  INTENT_UPDATE = 2;
}

// Field mapping configuration for protobuf transformation.
//
// Defines how to transform data between processing steps using various mapping strategies.
message ProcessingMapping {
  // Unique identifier for this mapping.
  string mapping_id = 1;
  // Source field paths to map from.
  //
  // For AGGREGATE, multiple paths are used. For other types, only the first is used.
  repeated string source_field_paths = 2;
  // Target field paths to map to.
  //
  // For SPLIT, multiple paths are used. For other types, only the first is used.
  repeated string target_field_paths = 3;
  // Type of mapping operation to perform.
  MappingType mapping_type = 4;
  // Configuration specific to the mapping type.
  //
  // Only one should be set, corresponding to mapping_type. DIRECT mappings require no config.
  oneof mapping_config {
    // Configuration for TRANSFORM mappings.
    TransformConfig transform_config = 5;
    // Configuration for AGGREGATE mappings.
    AggregateConfig aggregate_config = 6;
    // Configuration for SPLIT mappings.
    SplitConfig split_config = 7;
    // Configuration for CEL mappings.
    CelConfig cel_config = 8;
  }
}

// Type of field mapping operation.
enum MappingType {
  // Unspecified mapping type.
  MAPPING_TYPE_UNSPECIFIED = 0;
  // Direct field-to-field copy.
  MAPPING_TYPE_DIRECT = 1;
  // Apply transformation function to field.
  MAPPING_TYPE_TRANSFORM = 2;
  // Combine multiple source fields into one target.
  MAPPING_TYPE_AGGREGATE = 3;
  // Split one source field into multiple targets.
  MAPPING_TYPE_SPLIT = 4;
  // Calculate value using CEL expression.
  MAPPING_TYPE_CEL = 5;
}

// Configuration for field transformation mappings.
//
// Supports various transformation rules, including the high-performance "proto_rules"
// imperative script syntax for in-place protobuf manipulation.
message TransformConfig {
  // Name of the transformation rule to apply.
  //
  // Examples: "uppercase", "trim", "substring", "proto_rules"
  string rule_name = 1;
  // Optional parameters for the transformation rule.
  //
  // Example: for "substring", params could be {"start": 0, "end": 10}
  // For "proto_rules", use params {"rules": ["target = source", "-to_clear"]}
  optional google.protobuf.Struct params = 2;
}

// Configuration for CEL-based mappings.
message CelConfig {
  // The CEL expression to evaluate.
  //
  // The expression has access to the document and stream context.
  //
  // Available context variables:
  //   - document: The full PipeDoc object
  //   - stream: The PipeStream object (metadata, history, etc.)
  //
  // Example expressions:
  //   - "document.search_metadata.relevance_score * 100"
  //   - "document.search_metadata.title + ' - ' + document.search_metadata.author"
  //   - "stream.metadata.context_params['priority'] == 'high'"
  string expression = 1;
}

// Configuration for field aggregation mappings.
message AggregateConfig {
  // Type of aggregation operation.
  enum AggregationType {
    // Unspecified aggregation type.
    AGGREGATION_TYPE_UNSPECIFIED = 0;
    // Concatenate string values.
    AGGREGATION_TYPE_CONCATENATE = 1;
    // Sum numerical values.
    AGGREGATION_TYPE_SUM = 2;
  }
  // Type of aggregation to perform.
  AggregationType aggregation_type = 1;
  // Delimiter for concatenating strings.
  //
  // Example: " " to join with a space.
  optional string delimiter = 2;
}

// Configuration for field splitting mappings.
message SplitConfig {
  // Delimiter to use for splitting a string.
  string delimiter = 1;
}

// Node-specific processing configuration.
//
// Defines pre/post mappings, filtering, and custom configuration for a processing node.
message NodeProcessingConfig {
  // Node identifier for this configuration.
  string node_id = 1;
  // Field mappings applied before node processing.
  repeated ProcessingMapping pre_mappings = 2;
  // Field mappings applied after node processing.
  repeated ProcessingMapping post_mappings = 3;
  // Processing intent for this record.
  Intent intent = 4;
  // Node-specific custom configuration.
  google.protobuf.Any node_config = 5;
  // CEL expression to filter documents before node processing.
  //
  // When this expression evaluates to false, the document skips this node
  // and proceeds to the next node in the pipeline without processing.
  //
  // Available context variables:
  //   - document: The PipeDoc being processed
  //   - stream: The PipeStream object (metadata, history, etc.)
  //
  // Example expressions:
  //   - "document.search_metadata.language == 'en'"
  //   - "stream.metadata.context_params['skip_ocr'] == 'true'"
  //   - "document.search_metadata.content_length > 100"
  string filter_condition = 6;
  // Whether to save the document to the repository even if processing fails.
  //
  // When true, the document is persisted to the repository (S3) even if the node
  // processing encounters an error. This is useful for debugging and reprocessing
  // failed documents later.
  bool save_on_error = 7;
}

// Vector embedding for a document or document segment.
//
// Typically represents embeddings for whole documents or large non-chunk segments.
message Embedding {
  // Model identifier that generated this embedding.
  optional string model_id = 1;
  // Vector representation as floating-point values.
  repeated float vector = 2;
}

// Text content and vector embedding for a single chunk.
message ChunkEmbedding {
  // Actual text content of the chunk.
  string text_content = 1;
  // Vector embedding for this chunk's text.
  repeated float vector = 2;
  // Unique identifier for this chunk.
  optional string chunk_id = 3;
  // Character offset where chunk starts in original document.
  optional int32 original_char_start_offset = 4;
  // Character offset where chunk ends in original document.
  optional int32 original_char_end_offset = 5;
  // Identifier for a group of related chunks.
  optional string chunk_group_id = 6;
  // Identifier for the chunking configuration used.
  optional string chunk_config_id = 7;
}

// Single semantic chunk with its text and embedding.
message SemanticChunk {
  // Unique identifier for this chunk within its parent result.
  string chunk_id = 1;
  // Sequential number of this chunk within its parent result.
  int64 chunk_number = 2;
  // Text and embedding information for this chunk.
  ChunkEmbedding embedding_info = 3;
  // Chunk-specific metadata (e.g., page number, section).
  map<string, google.protobuf.Value> metadata = 4;
}

// Complete result of semantic chunking and embedding process.
//
// Represents one specific processing configuration applied to a PipeDoc field.
// A PipeDoc can contain multiple such results from different configurations.
message SemanticProcessingResult {
  // Unique identifier for this result set.
  string result_id = 1;
  // Name of the PipeDoc field that was processed.
  //
  // Examples: "body", "title", "abstract"
  string source_field_name = 2;
  // Identifier for the chunking configuration used.
  //
  // Examples: "sentence_splitter_v1", "token_chunker_512_overlap_50"
  string chunk_config_id = 3;
  // Identifier for the embedding model/configuration used.
  //
  // Examples: "ada_002_v1", "minilm_l6_v1"
  string embedding_config_id = 4;
  // Generated name for this result set.
  //
  // Used as field name prefix in search indexes or for UI display/selection.
  // Examples: "body_chunks_ada_002", "title_sentences_minilm"
  optional string result_set_name = 5;
  // List of semantic chunks with embeddings from this configuration.
  repeated SemanticChunk chunks = 6;
  // Processing run metadata (e.g., model version, execution time).
  map<string, google.protobuf.Value> metadata = 7;
}

// Execution record for a single pipeline processing step.
//
// Tracks the execution details, timing, status, and any errors for a step.
message StepExecutionRecord {
  // Sequential hop number for this step in the processing stream.
  int64 hop_number = 1;
  // Name of the pipeline step configuration that was executed.
  string step_name = 2;
  // Identifier of the service instance/pod that executed the step.
  optional string service_instance_id = 3;
  // Timestamp when step processing began.
  google.protobuf.Timestamp start_time = 4;
  // Timestamp when step processing ended.
  google.protobuf.Timestamp end_time = 5;
  // Execution outcome status.
  //
  // Expected values: "SUCCESS", "FAILURE", "SKIPPED", "RETRYING", "DISPATCH_FAILURE"
  string status = 6;
  // Logs from the processor for this step's execution.
  repeated string processor_logs = 7;
  // Error information if status is "FAILURE".
  optional ErrorData error_info = 8;
  // Target step name if status is "DISPATCH_FAILURE".
  optional string attempted_target_step_name = 9;
}

// Error information for a failed processing step.
//
// Captures error details, timing, and input state for debugging and reproducibility.
message ErrorData {
  // Human-readable error description.
  string error_message = 1;
  // Machine-readable error code.
  //
  // Examples: "CONFIG_VALIDATION_ERROR", "TIMEOUT_ERROR", "PARSE_ERROR"
  optional string error_code = 2;
  // Technical details such as stack trace snippets.
  optional string technical_details = 3;
  // Name of the step where the error originated or was detected.
  string originating_step_name = 4;
  // Target step name if error occurred during routing/dispatch.
  optional string attempted_target_step_name = 5;
  // Input state at the time of failure for reproducibility.
  optional FailedStepInputState input_state_at_failure = 6;
  // Timestamp when the error occurred or was logged.
  google.protobuf.Timestamp timestamp = 7;
}

// Snapshot of input state when a processing step failed.
//
// Used for debugging and reproducing failures by capturing the exact state.
message FailedStepInputState {
  // PipeDoc state before the failed step was attempted.
  optional PipeDoc doc_state = 1;
  // Blob state before the failed step was attempted.
  optional Blob blob_state = 2;
  // Custom JSON configuration provided to the failed step.
  optional google.protobuf.Struct custom_config_struct = 3;
  // Configuration parameters provided to the failed step.
  map<string, string> config_params = 4;
}

// Configuration for a processing step instance.
//
// Contains the custom JSON configuration and key-value parameters for a module.
// Used by both graph nodes (in config) and module requests (in module service).
message ProcessConfiguration {
  // Structured JSON configuration for the module.
  //
  // This is the primary configuration mechanism for complex module settings.
  // Frontend uses JSONForms with the module's json_config_schema to generate UI.
  google.protobuf.Struct json_config = 1;
  // Simple key-value configuration parameters.
  //
  // Used for simple overrides, environment-specific values, or when no schema exists.
  // Can be used alongside json_config for runtime parameters.
  map<string, string> config_params = 2;
}

// Dead Letter Queue message for failed processing attempts.
//
// Published by Sidecar when Engine returns error after retry exhaustion.
// Contains the failed stream, error context, and Kafka offset information for replay.
message DlqMessage {
  // The failed stream (contains document reference).
  PipeStream stream = 1;
  // Error classification.
  //
  // Examples: "TIMEOUT", "CONNECTION_REFUSED", "UNAVAILABLE", "MODULE_UNAVAILABLE"
  string error_type = 2;
  // Human-readable error message.
  string error_message = 3;
  // Timestamp when the failure occurred.
  google.protobuf.Timestamp failed_at = 4;
  // Number of retry attempts that were made before sending to DLQ.
  int32 retry_count = 5;
  // Node ID where the failure occurred.
  string failed_node_id = 6;
  // Original Kafka topic for replay correlation.
  string original_topic = 7;
  // Original Kafka partition for replay correlation.
  int32 original_partition = 8;
  // Original Kafka offset for replay correlation.
  int64 original_offset = 9;
  // Number of times this message has been reprocessed from the DLQ.
  //
  // Incremented each time the message is replayed from DLQ and fails again.
  // Used to prevent infinite retry loops for poison messages.
  // When this exceeds the configured max_dlq_reprocess_count, the message
  // should be quarantined or logged for manual review.
  int32 dlq_reprocess_count = 10;
  // Whether this message has been quarantined (exceeded max reprocess attempts).
  //
  // Quarantined messages should not be automatically reprocessed.
  // Requires manual intervention to investigate and fix the root cause.
  bool quarantined = 11;
  // Timestamp when the message was quarantined (if applicable).
  google.protobuf.Timestamp quarantined_at = 12;
}
